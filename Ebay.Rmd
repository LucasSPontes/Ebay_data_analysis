---
title: "**Data Preparation & Machine Learning with Ebay Shill Bidding data**"
author: "Lucas Pontes"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 2
---

**[SBD Dataset Web Page](https://archive.ics.uci.edu/ml/datasets/Shill+Bidding+Dataset)**

```{r, set.seed(333), include = FALSE}
knitr::opts_chunk$set(cache = T)
library(readr)
library(dplyr)
library(naniar)
library(treemapify)
library(ggplot2)
library(reshape2)
library(viridis)
library(tidyverse)
library(hrbrthemes)
library(knitr)
library(MASS)
library(caret)
library(cluster)
library(factoextra)
library(dendextend)
library(caTools)
library(randomForest)
library(class)
library(tree)
library(rpart)
library(rpart.plot)
library(pROC)
library(DAAG)
library(party)
library(mlbench)
SBD <- read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00562/Shill%20Bidding%20Dataset.csv")
```


# **Introduction**

The Shill Bidding Data set `SBD` comprise eBay auctions with many different features, including the duration of the auctions, bidder tendency and class, among others (Alzahrani and Sadaoui, 2018). The aim of this report is to submit the data to different supervised and unsupervised machine learning methods after properly characterisation and preparation of the dataset. To achieve the best result a scaling method will be applied alongside feature reduction methods, machine-learning methods applied will have the performance and accuracy level measured and compared. At the end of this report, the supervised and  unsupervised methods will be identified in order to work with optimal performance in the Shill Bidding Dataset. The predictions related to normal and abnormal bidding behaviour of eBay users may help companies to identify scams and other undesirable users within the platform.

# **Data preparation**

## **Data characterisation**

Data characterisation is defined as the pre-processing phase of summarization of the different features and characteristics presents in the observations of a given data set, this process may be accomplished by introducing the data to the viewer using statistics measure summaries, and presenting visually using graphs, like bar charts and scatter plots (Capozzoli, Cerquitelli and Piscitelli, 2016; Han, Kamber and Pei, 2012).



```{r, echo = FALSE}
SBD
glimpse(SBD)
summary(SBD)
```

A first glance at `SBD` shows 6321 observations for 13 features, the first three columns represent the record ID, auction and the bidder respectively. The dplyr glimpse function displays all columns beside bidder ID as numeric, however, class and all IDs columns should be treated as a character class.
  
  
```{r include = FALSE}
SBD$Class<-as.factor(SBD$Class)
SBD$Record_ID<-as.character(SBD$Record_ID)
SBD$Auction_ID<-as.character(SBD$Auction_ID)
SBD$Bidder_ID<-as.character(SBD$Bidder_ID)
```
  
```{r, echo = FALSE}
SBD
summary(SBD)
```
  
The summary function exhibits some general descriptive statistics, it is noticeable that the data have been pre-processed as auction duration range from 0 to 10 and all other numerical features range from 0 to 1.

## **Exploratory Data Analysis**

```{r, echo = FALSE,fig.width = 8, fig.height = 4}
SBD
vis_miss(SBD)
```

As displayed by the graphic generated by naniar function "vis_miss" on `SBD`, the data does not include any standard missing value.


```{r,include = FALSE}
prop <- SBD %>%
  count(Class)
```

```{r,echo = FALSE,fig.width = 9, fig.height = 2}
ggplot(prop, 
       aes(fill = Class, 
           area = n,
           label = c("Normal", "Abnormal"))) +
  geom_treemap() + 
  geom_treemap_text(colour = "white", 
                    place = "centre") +
  labs(title = "Proportion of normal and abnormal bidding behaviour")
```

```{r,echo = FALSE}
table(SBD$Class)
round(prop.table(table(SBD$Class)),digits=2)
```

  As displayed by the treemap and proportion table above, only 10.7% (675 observations) of the biddings on `SBD` are considered abnormal. The graph above does a great job showing the proportion. The conventional use of pie charts is not recommended to display proportion, especially when the feature contains multiple unique observations or small fractions, Human perception does not comprehend angular proportions in pie charts as it should (Hunt, 2019). 


```{r include = FALSE}
SBD_matrix <- as.matrix(SBD[, c(4:11)])
corr_mat <- round(cor(SBD_matrix),2)
melted_corr_mat <- melt(corr_mat)
melt_SBD <- melt(SBD[,4:11])
```


```{r,echo = FALSE,fig.width = 8, fig.height = 8}
ggplot(data = melted_corr_mat, aes(x=Var1, y=Var2,
                                   fill=value)) +
geom_tile() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
scale_fill_viridis(discrete=FALSE) +
geom_text(aes(Var2, Var1, label = value),
          color = "black", size = 4) +
  labs(title = "Correlation Analysis") +
xlab("") +
ylab("")
```

The heatmap above shows the correlation between all numerical features in a single graph. The problem with heatmaps is that slight colour intensity variations can be hard to perceive by human eyes, especially with certain colour combinations. Approximately 8% of men and 0.5% of women have some form of colourblindness (BioTuring’s Blog, 2018; Colour Blind Awareness, n.d.). The ggplot graph above has a colourblind friendly colour palette and an added numerical scale, allowing the viewer to check the exact correlation between the variables, as an example, the positive correlation between early and last bidding features is notorious. Another noticeable correlation is the negative one between the winning ratio and auction bids, for further investigation, both will be displayed in a scatter plot.

```{r, warning=FALSE,echo = FALSE}
ggplot(SBD, aes(x = Early_Bidding, y=Last_Bidding )) + 
    geom_point(size=2,alpha= 0.3) +
    theme_ipsum() +
    ggtitle("Correlation between Early and Last Bidding")
```


The scatter plot makes it noticeable why those two variables are so positive correlated, considering that the last bidding value can never be lower than the early bidding value. A positive slope can be found in the diagonal centre of the 2-dimensional plot, this line represents the values which were the same in both features.

```{r, warning=FALSE,echo = FALSE}
ggplot(SBD, aes(x = Winning_Ratio, y=Auction_Bids )) + 
    geom_point(size=2,alpha= 0.3) +
    theme_ipsum()+
    ggtitle("Correlation between winning ratio and auction bids")
```

Different from the previous scatter plot, this time the features auction bids and winning ratio, which had a negative correlation in the heatmap representation, do not show any visible correlation. This interaction will be later tested when used as a predictor in a linear regression method.  
  
  
```{r, warning=FALSE,echo = FALSE}
ggplot(SBD, aes(x= as.numeric(Record_ID))) +
  geom_histogram( binwidth=300, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  ggtitle("Record ID distribution") +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))

ggplot(SBD, aes(x= as.numeric(Auction_ID))) +
  geom_histogram( binwidth=60, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  ggtitle("Auction ID distribution") +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))
```

The histograms above display a clear difference between the distribution of the Auction and Record ID, as the x-axis of the Auction ID, is much shorter, this happens because many auctions are represented in the data set multiple times. Also, the Record ID displays some uniformity, even so not perfectly uniform, representing the absence of multiple observations with the same ID.

```{r, warning=FALSE,echo = FALSE,fig.width = 9}
ggplot(melt_SBD, aes(x=variable, y=value, fill=variable )) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  theme_ipsum() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ggtitle("Boxplot for multiple variables") +
  xlab("")
```

The Boxplot above shows the distribution of several numerical features with a range between 0 and 1. These 8 features were chosen to be represented by grouped boxplots since it is an exceptional alternative to viewing many distributions in a single graph, especially if the y-axis range matches all features. This figure is extremely informative, considering that it displays the distribution of the observations, the quartile range including the median, and the minimum and maximum values, which describe the range (Galarnyk, 2018).


```{r, warning=FALSE,echo = FALSE}
ggplot(SBD, aes(x= Auction_Duration)) +
  geom_histogram( binwidth=1, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  ggtitle("Auction Duration distribution") +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))
```

Despite being a numerical feature, the low number of distinct values causes the histogram to not be aesthetically pleasing, however, it shows the 5 distinct values 7,3,1,5 and 10 and how many times each one appeared in the auction duration column. This pattern suggests five fixed auction durations options that the platform allows the sellers to choose from.

```{r, results = 'asis',echo = FALSE}
SBD[,4:13] %>%
pivot_wider(names_from = Class,
            values_from = c("Bidder_Tendency", "Bidding_Ratio", "Successive_Outbidding", "Last_Bidding","Auction_Bids", 
                            "Starting_Price_Average" , "Early_Bidding" ,
                            "Winning_Ratio" , "Auction_Duration" ), values_fn = ~mean(.x)) %>%
as.data.frame() %>% 
t() %>%
kable(format = "html")
```

The pivot table above points to a substantial difference in the group means value for most features when compared to normal and abnormal bidding patterns. This information is important since the class feature will be the dependable variable for machine learning models in the next steps.

## **Data cleaning**

The importance of identifying and removing missing values from the data prior to modelling is that it can reduce its accuracy and cause bias in the analysis (Analytics Vidhya, 2021). As shown before, there is not a single missing value in the data. However, sometimes the missing values appear as non-standard, a different format that includes strings named "NA", "n/a", "--" and many other possibilities. Unfortunately, it is near-impossible to check for all of them, especially in datasets with numerous observations and features. When it happens the feature type can only be categorical, since 0 in numeric features should not be considered meaningless (Sullivan, 2018). This section aims to check and remove non-standard missing values that may be present in the "Bidder_ID" column.

```{r,echo = FALSE}
SBD <- SBD %>%
  mutate(Bidder_ID = replace(Bidder_ID, Bidder_ID == "na", NA)) %>%
  mutate(Bidder_ID = replace(Bidder_ID, Bidder_ID == "N/A", NA)) %>%
  mutate(Bidder_ID = replace(Bidder_ID, Bidder_ID == "--", NA)) %>%
  mutate(Bidder_ID = replace(Bidder_ID, Bidder_ID == "n/a", NA)) 

sum(is.na(SBD$Bidder_ID))
```

Checking for common strings associated with non-standard missing values, the result of the analysis indicates the complete absence of them.

## **Feature engineering**

As an important stage preceding the machine learning modelling, this task requires the judgement of the data analyst and numerous trials and errors. Feature engineering is the act of transforming a given feature space with different tools, especially mathematical, with the main goal of improving the model performance (Khurana, Samulowitz and Turaga, 2018).The first step will be removing unnecessary columns for the future tasks, the first three columns only include the IDs for the record, auction and bidder respectively and do not add any significance to the analysis.

```{r,echo = FALSE}
SBD <- as.data.frame(SBD[,4:13])
head(SBD)
```


As observed by the correlation of early and last biding in the scatter plot, the early bidding can never exceed the last bidding in value, however, what eventually happens is that the auction has no increase in price and culminate in the same value. To join the information of the two columns in one and add extra information to the future predictive model, a new column named "Bidding difference" will substitute them with the existing incremented value calculated by the difference between their prices.

```{r,echo = FALSE}
SBD$Bidding_difference <- SBD$Last_Bidding - SBD$Early_Bidding
SBD <- as.data.frame(SBD[, c(-4,-7) ])
SBD <- SBD[order(SBD$Bidding_difference),]
head(SBD,30)
```

Calculating the difference between last and early bidding resulted in a feature with 25 negative values unexpected, suggesting that in these 25 observations the trade was concluded with the last bidding having a lower value than the early bidding, which is counterintuitive.

```{r, warning=FALSE,echo = FALSE}
SBD$Auction_Duration <- SBD$Auction_Duration / 10
ggplot(SBD, aes(x= Auction_Duration)) +
  geom_histogram( binwidth=0.1, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  ggtitle("Auction Duration distribution") +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))
```

Since all other metrics range from 0 to 1, dividing the auction duration variable by 10 puts it in the same range. This may reduce the chance of this variable impacting the model disproportionately.

## **Data scaling**

The different orders of magnitude may bias the results since most machine learning algorithms calculate the output using Euclidean distance (Asaithambi, 2017). As seen in the distribution of the previous boxplots and histograms the numeric features contain some outliers, considering the putative relevance of these outliers in the machine learning analysis, the ideal transformation method will maintain the impact of these feature outliers in the output of the machine learning algorithm to be performed. The two regularly used scaling methods are standardization and normalization (GeeksforGeeks, 2020).

```{r, warning=FALSE,echo = FALSE,fig.width = 9}
melt(SBD[,-8]) %>%
ggplot(aes(x=variable, y=value, fill=variable )) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  theme_ipsum() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ggtitle("Independent variables boxplot") +
  xlab("")
```

As displayed before, except for a few negative values in the Bidding difference column, all features range from 0 to 1, the data was probably been already pre-processed. Data scaling is necessary when the features of a dataset have a different range or order of magnitude (Roy, 2020), which is not the case for both, taking that and the fact that the outliers may be important for the analysis, the data scaling will not be employed.

# **Dimensionality reduction**

In pattern classification using a Machine Learning algorithm, the training vectors are evaluated by comparing them to the test split, a common problem that can reduce its accuracy is the large dimensionality of the data, and numerous features reducing the robustness of the pattern classifiers (Sharma and Paliwal, 2014). 
The main distinction between the two methods applied is that LDA is supervised, requiring labelled inputs, while PCA is unsupervised, ignoring class labels. LDA basis on discovering the feature subspace of highest separability between classes, while PCA focus in reproduce the topmost direction of variation in the features, the first component (PC1) accounting for the highest variation and the PC2 the second-best and so forth (Duvva, 2021).

## **Principal Component Analysis (PCA)**

Principal component analysis or PCA is an unsupervised technique, ignoring class labels. The main objective of performing a PCA prior to the Machine Learning technique is to increase interpretability while preserving statistical information (Jolliffe and Cadima, 2016).

```{r, warning=FALSE,echo = FALSE}
SBD.PCA <- prcomp(x = SBD[,-8], 
       scale = FALSE,
       center = TRUE)
summary(SBD.PCA)
```

The analysis resulted in 8 principal components, the first two explain 70% of the total variance. Five components will be applied for the upcoming clustering algorithms, considering that this amount is enough to describe over 93% of the total variance (Hayden, 2018).


```{r, warning=FALSE,include = FALSE}
library(ggbiplot)
```

```{r, warning=FALSE,echo = FALSE, fig.width = 7, fig.height = 8}
ggbiplot(SBD.PCA)
SBD_transform = as.data.frame(-SBD.PCA$x[,1:5])
```

Originating from the centre point of the graph above the arrows indicate the contribution of each variable to the first two principal components. As seen, the variables “Auction Bids” and “Starting Price Average”  contributes with high values to the first principal component, while “Successive Outbidding” and “Bidder Tendency” to the second principal component (Hayden, 2018).


## **Linear Discriminant Analysis (LDA)**

Another technique used to approach the high-dimensionality problem is the Linear discriminant analysis or LDA, which different from PCA, is a supervised technique that generally exceeds other techniques when applied to feature classification, LDA method reduces the number of features that fit in the different classes prior to classification while maintaining the classes well separated in this lower dimensional space (Sharma and Paliwal, 2014).


```{r, warning=FALSE,echo = FALSE}
split <- sample.split(SBD$Class, SplitRatio = 0.8)
training_set <- subset(SBD, split == TRUE)
test_set <- subset(SBD, split == FALSE)

lda <- lda(formula = Class ~., data = training_set)
lda
training_set_lda <- as.data.frame(predict(lda, training_set))
training_set_lda <- training_set_lda[c(4, 1)]
test_set_lda  <- as.data.frame(predict(lda, test_set)) 
test_set_lda  <- test_set_lda [c(4, 1)]
```

The training data displays similar proportions of normal and abnormal bidding behaviour found in the SBD data, 90% and 10% respectively, including the similar group means values obtained in the exploratory data analysis section. Represented by the Coefficients of linear discriminants, these values indicate the predictor’s linear combination for the different features of the data applied in the decision rule in the LDA model. Since the class feature only contains two unique labels the analysis results in a single linear discriminant, LD1 (Zach, 2020). 

```{r, warning=FALSE,echo = FALSE}
colours <- c("red" , "blue")
ggplot(training_set_lda, aes(x=LD1, y = 0, color=class)) + 
    geom_point(size=4) +
    theme_ipsum() +
    scale_color_manual(values = colours)
```


```{r, warning=FALSE,echo = FALSE}
ggplot(training_set_lda, aes(x=LD1, y = 0, color=class)) +
  geom_point(size = 5.5)  +
  annotate("segment",x=-1.5,xend=8, y=0, yend=0, size=1.7) +
  annotate("segment",x=-1.5,xend=-1.5, y=-0.1,yend=0.1, size=1.7) +
  annotate("segment",x=8 ,xend=8, y=-0.1,yend=0.1, size=1.7)+
  scale_x_continuous(limits = c(-1.5,8)) +
  scale_y_continuous(limits = c(-1,1)) +
  scale_color_manual(values = colours) + 
  theme(panel.background = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())
```


```{r, warning=FALSE,echo = FALSE}
ldahist(data = training_set_lda$LD1, g = training_set_lda$class)
```

LD1 does a great job separating the different classes in a single dimension, there are almost no overlaps between normal and abnormal bidding behaviour (finnstats, 2021). This result suggests high predictive accuracy in the classification algorithms that will be applied in the following sections.

# **Machine Learning**

The concept of Machine Learning is a measurable improvement in a computer program's function as it performance improves as it is presented with more data. Machine Learning algorithm are built to handle three different types of task, clustering, regression and classification. To solve these tasks the analyst has to decide using his experience the appropriate method to select between reinforcement learning, supervised learning, unsupervised learning, and semi-supervised learning, considering that each method is pertinent to different types of data (Ray, 2019).

## **Clustering algorithms**

Clustering is a valuable set of methods used in many applications, these unsupervised algorithms divide the features into classes, this division is based on data patterns identified by the algorithm, and the objects are grouped on the defined class based on similar behaviour (Dubey and Choubey, 2017). For the following algorithms the also unsupervised PCA scaled data will be applied, the reason is that the scaled data still have a high performance, even though being lower compared to the other methods, but the sparse distribution of the components contributes to better visualization.


### **K-Means Clustering**

This clustering algorithm is the most widely employed, the goal of K-Means unsupervised technique is to partition the observations into k clusters so the observations can be grouped based on the closest mean (Dubey and Choubey, 2017).
  
  
```{r, warning=FALSE,echo = FALSE}
fviz_nbclust(SBD[,-8], kmeans, method = 'gap_stat') +
labs(title= "SBD optimal number of clusters")
```

Elbow and silhouette methods only measure global clustering elements, as a more refined method gap statistics formalize these methods in a heuristic statistical in order to evaluate the ideal number of cluster that should be applied in the k-means algorithm (Tibshirani, Walther and Hastie, 2001).

```{r, warning=FALSE,echo = FALSE}
kmeans_SBD = kmeans(SBD[,-8], centers = 4, nstart = 50)
fviz_cluster(kmeans_SBD, data = SBD[,-8]) +
labs(title= "SBD Cluster plot")
``` 
  
Setting four different clusters better identify the nature of the features related to normal and abnormal observations. The scatter plot displays the separation of the clusters by colour, each colour associated with one cluster. As we know, based on the class feature, which is not being used on unsupervised algorithms like k-means and hierarchical agglomerative clustering. The data is supposed to be divided into two categories, however, the normal bidding behaviour has a way bigger proportion than abnormal behaviour, thus being subjected to multiple patterns within itself (Maklin, 2019).

  
```{r, warning=FALSE,echo = FALSE}
fviz_nbclust(SBD_transform, kmeans, method = 'gap_stat') +
labs(title= "PCA transformed SBD optimal number of clusters")
```

The ideal number of clusters for LDA transformed data is also four, as displayed in the gap statistical method above (Tibshirani, Walther and Hastie, 2001).

```{r, warning=FALSE,echo = FALSE}
kmeans_SBD_PCA = kmeans(SBD_transform, centers = 4, nstart = 50)
fviz_cluster(kmeans_SBD_PCA, data = SBD_transform)+
labs(title= "PCA transformed SBD Cluster plot")
```

As exhibited in the scatter plot above the PCA transformed data achieve a fantastic job dividing the data into 4 different clusters, mainly on the x-axis.

### **Hierarchical Agglomerative Clustering**

As with other multivariate methods, the implemented hierarchical agglomerative clustering algorithm is an essential tool for the embedded classification of numerical features using sets of variables. This method is usually applied when the data has a large number of partitions each one correlated with a level of the hierarchy (Murtagh and Contreras, 2017).


```{r, warning=FALSE,echo = FALSE, fig.width = 14, fig.height = 9}
dist_mat <- dist(SBD[,-8], method = 'euclidean')
hclust_avg <- hclust(dist_mat, method = 'average')
```


```{r, warning=FALSE,echo = FALSE, fig.width = 14, fig.height = 9}
avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend <- color_branches(avg_dend_obj, h = 1.2)
plot(avg_col_dend, main = "SBD Cluster Dendogram" ) 
```

The dendrogram graphic representation above represents the number of partitions within the data. Asobserved, the ideal number of clusters is also four, as obtained in the k-means section, this is noticeable by inspecting the vertical axis (Maklin, 2019).
  
```{r, warning=FALSE,echo = FALSE, fig.width = 14, fig.height = 9}
dist_mat_PCA <- dist(SBD_transform, method = 'euclidean')
hclust_avg_PCA <- hclust(dist_mat_PCA, method = 'average')
```


```{r, warning=FALSE,echo = FALSE, fig.width = 14, fig.height = 9}
avg_dend_obj_PCA <- as.dendrogram(hclust_avg_PCA)
avg_col_dend_PCA <- color_branches(avg_dend_obj_PCA, h = 1.1)
plot(avg_col_dend_PCA, main = "PCA transformed SBD Cluster Dendogram")
```

Both K-Means and Hierarchical Agglomerative Clustering exhibited similar results, indicating four patterns, being the normal data with three different patterns within itself and the abnormal data in a different fourth fashion, as suggested by the respective diagnostic plots. Both methods did an amazing job identifying and discriminating the different categories.

## **Classification algorithms**

  Applied in both structured and unstructured data, classification algorithms aim is to learn from training data in order to label new data into a specific class (Sen, Hajra and Ghosh, 2019).


### **Random Forest Classification**

  The Random Forest Classification technique can be applied in both regression and classification, this supervised learning methodology performs well with numerous and imbalanced datasets, dealing with missing data efficiently (More and Rana, 2017).

```{r, warning=FALSE,echo = FALSE}
SBD.rf <- randomForest(Class ~., 
                        data = training_set, 
                        importance = TRUE,
                        proximity = TRUE)
print(SBD.rf)
```

```{r, warning=FALSE,echo = FALSE}
plot(SBD.rf)
```

```{r, warning=FALSE,echo = FALSE}
LDASBD.rf <- randomForest(class ~ ., 
                        data = training_set_lda, 
                        importance = TRUE,
                        proximity = TRUE)
print(LDASBD.rf)
```

```{r, warning=FALSE,echo = FALSE}
plot(LDASBD.rf)
```
  
### **K-Nearest Neighbours**

  Mostly used as a classifier, K-Nearest Neighbours or KNN method do its calculations using euclidean distance. This supervised non-parametric method is known for its simplicity and efficacy (Taunk et al., 2019).
  
```{r, warning=FALSE,echo = FALSE} 
classifier_knn <- knn(train = training_set,
                      test = test_set ,
                      cl =  training_set$Class,
                      k = 5)

cm <- table(test_set$Class, classifier_knn)
cm
  

misClassError <- mean(classifier_knn != test_set$Class)
print(paste('Accuracy =', 1-misClassError))
```

```{r, warning=FALSE,echo = FALSE} 
classifier_knn_lda <- knn(train = training_set_lda,
                      test = test_set_lda ,
                      cl =  training_set_lda$class,
                      k = 5)
  
cm_lda <- table(test_set_lda$class, classifier_knn_lda)
cm_lda
  
misClassError_lda <- mean(classifier_knn_lda != test_set_lda$class)
print(paste('Accuracy =', 1-misClassError_lda))
```


### **Decision Tree**

  Another supervised method, decision tree method combines a sequence of straightforward techniques, in which each one makes use of a threshold value as a metric against possible values, this method classifies the data that fall in a particular region as integrated to the most recurrent class in that region. The decision tree's main goal is to depict a tree with its findings data patterns accomplished by checking which test best separate the different cases into classes (Kotsiantis, 2011).


```{r, warning=FALSE,echo = FALSE} 
tree.SBD <- rpart(Class ~., data = training_set)
rpart.plot(tree.SBD)
```

```{r, warning=FALSE,echo = FALSE} 
printcp(tree.SBD)
plotcp(tree.SBD)
```

```{r, warning=FALSE,echo = FALSE} 
p <- predict(tree.SBD, training_set, type = 'class')
confusionMatrix(p, training_set$Class )
```


```{r, warning=FALSE,echo = FALSE}
p1 <- predict(tree.SBD, test_set, type = 'prob')
p1 <- p1[,2]
r <- multiclass.roc(test_set$Class, p1, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,
         print.auc=TRUE,
         auc.polygon=TRUE,
         grid=c(0.1, 0.2),
         grid.col=c("green", "red"),
         max.auc.polygon=TRUE,
         auc.polygon.col="lightblue",
         print.thres=TRUE,
         main= 'ROC Curve')
```

```{r, warning=FALSE,echo = FALSE} 
tree.LDA <- rpart(class ~ .,data = training_set_lda)
rpart.plot(tree.LDA)
```

```{r, warning=FALSE,echo = FALSE} 
printcp(tree.LDA)
plotcp(tree.LDA)
```

```{r, warning=FALSE,echo = FALSE} 
p <- predict(tree.LDA, training_set_lda, type = 'class')
confusionMatrix(p, training_set_lda$class )
```


```{r, warning=FALSE,echo = FALSE}
p1 <- predict(tree.LDA, test_set_lda, type = 'prob')
p1 <- p1[,2]
r <- multiclass.roc(test_set_lda$class, p1, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,
         print.auc=TRUE,
         auc.polygon=TRUE,
         grid=c(0.1, 0.2),
         grid.col=c("green", "red"),
         max.auc.polygon=TRUE,
         auc.polygon.col="lightblue",
         print.thres=TRUE,
         main= 'ROC Curve')
```

### **Comparison of ML algorithms for Classification**

Every single supervised algorithm resulted in an outstanding performance, Random Forest Classification, K-Nearest Neighbours and Decision Tree resulted in an accuracy score of over 97%, with minor differences between them. K-Nearest Neighbours had the best accuracy score, however, the difference is negligible, it is safe to say that the predictions of all three methods are suitable.



# **Conclusion**

  As a result of the comparison between scaled methods, it was observed that, except for the PCA, the standard scaling did not affect the result of the predictions, yet the scaling sparsed the data and allowed its observation to be better displayed in the diagnostic plots. Scaled PCA resulted in a more sparsed and two-dimensional plot, nevertheless, the LDA still had a better accuracy score when applied to random forest classification, being the method of choice for following classification algorithms. 
  
  Three supervised algorithms were implemented for classification purposes, Random Forest, K-Nearest Neighbours and Decision Tree. Every single one of these algorithms resulted in an outstanding accuracy score, being excellent to make predictions for the normal and abnormal bidding behaviour, especially normal, because of the massive amount of observation that falls into this category. For the unsupervised algorithms, two clustering methods were applied using the scaled PCA data, K-Means Clustering and Hierarchical Agglomerative Clustering. It was observed in both algorithms clear division of the data into three distinct patterns, being two of them associated with normal bidding behaviour and one with abnormal.
  
   A Further step to improve the prediction power of these models for the detection of abnormal behaviour, that may be associated with scams and other unwanted conducts, could be adding more data with feature values associated with the abnormal bidding pattern observed in the algorithms implemented. Considering the results, it is conclusive that all implemented algorithms performed extremely well, specially K-nearest neighbours regression with feature reduced data by LDA.


# **References**

finnstats (2021). Linear Discriminant Analysis in R | R-bloggers. [online] Available at: https://www.r-bloggers.com/2021/05/linear-discriminant-analysis-in-r/.

Hayden, L.(2018). R PCA Tutorial (Principal Component Analysis). [online] Available at: https://www.datacamp.com/tutorial/pca-analysis-r [Accessed 16 Jun. 2022].

Tibshirani, R., Walther, G. and Hastie, T. (2001). Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), [online] 63(2), pp.411–423. doi:10.1111/1467-9868.00293.

Zach (2020). Linear Discriminant Analysis in R (Step-by-Step). [online] Statology. Available at: https://www.statology.org/linear-discriminant-analysis-in-r/.
