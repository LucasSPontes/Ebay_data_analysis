---
title: "**Data Preparation & Machine Learning with Ebay Shill Bidding data**"
author: "Lucas Pontes"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  github_document:
    toc: true
    toc_depth: 2
---

```{r data, include = FALSE}
library(readr)
library(dplyr)
library(naniar)
library(treemapify)
library(ggplot2)
library(reshape2)
library(viridis)
library(tidyverse)
library(hrbrthemes)
library(knitr)
SBD <- read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00562/Shill%20Bidding%20Dataset.csv")
```

# **Introduction**

  The Shill Bidding Data set `SBD` comprise eBay auctions with many different features, including the duration of the auctions, bidder tendency and class, among others (Alzahrani and Sadaoui, 2018). The aim of this report is to submit the data to different supervised and unsupervised machine learning methods after properly characterisation and preparation of the dataset. To achieve the best result a scaling method will be applied and compared alongside feature reduction methods, machine-learning methods applied will also have the performance and accuracy level measured and compared. At the end of this report, the supervised and  unsupervised methods will be identified in order to work with optimal performance in the Shill Bidding Dataset. The predictions related to normal and abnormal bidding behaviour of eBay users may help companies to identify scams and other undesirable users within the platform.



# **Data preparation**

## **Data characterisation**

  Data characterisation is defined as the pre-processing phase of summarization of the different features and characteristics presents in the observations of a given data set, this process may be accomplished by introducing the data to the viewer using statistics measure summaries, and presenting visually using graphs, like bar charts and scatter plots (Capozzoli, Cerquitelli and Piscitelli, 2016; Han, Kamber and Pei, 2012).



```{r}
SBD
glimpse(SBD)
summary(SBD)
```

  A first glance at `SBD` shows 6321 observations for 13 features, the first three columns represent the record ID, auction and the bidder respectively. The dplyr glimpse function displays all columns beside bidder ID as numeric, however, class and all IDs columns should be treated as a character class.
  
  
```{r include = FALSE}
SBD$Class<-as.factor(SBD$Class)
SBD$Record_ID<-as.character(SBD$Record_ID)
SBD$Auction_ID<-as.character(SBD$Auction_ID)
SBD$Bidder_ID<-as.character(SBD$Bidder_ID)
```
  
```{r}
summary(SBD)
```
  
  The summary function exhibits some general descriptive statistics, it is noticeable that the data have been pre-processed as auction duration range from 0 to 10 and all other numerical features range from 0 to 1.

## **Exploratory Graphs**

```{r}
vis_miss(SBD)
```

  As displayed by the graphic generated by naniar function "vis_miss" on `SBD`, the data does not include any standard missing value.


```{r include = FALSE}
prop <- SBD %>%
  count(Class)
```

```{r}

ggplot(prop, 
       aes(fill = Class, 
           area = n,
           label = c("Normal", "Abnormal"))) +
  geom_treemap() + 
  geom_treemap_text(colour = "white", 
                    place = "centre") +
  labs(title = "Proportion of normal and abnormal bidding behaviour")

```

```{r}
table(SBD$Class)
round(prop.table(table(SBD$Class)),digits=2)
```

  As displayed by the treemap and proportion table above, only 10.7% (675 observations) of the biddings on `SBD` are considered abnormal. The graph above does a great job showing the proportion. The conventional use of pie charts is not recommended to display proportion, especially when the feature contains multiple unique observations or small fractions, Human perception does not comprehend angular proportions in pie charts as it should (Hunt, 2019). 


```{r include = FALSE}
SBD_matrix <- as.matrix(SBD[, c(4:11)])
corr_mat <- round(cor(SBD_matrix),2)
melted_corr_mat <- melt(corr_mat)
melt_SBD <- melt(SBD[,4:11])
```


```{r}
ggplot(data = melted_corr_mat, aes(x=Var1, y=Var2,
                                   fill=value)) +
geom_tile() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
scale_fill_viridis(discrete=FALSE) +
geom_text(aes(Var2, Var1, label = value),
          color = "black", size = 4) +
  labs(title = "Correlation Analysis") +
xlab("") +
ylab("")
```

  The heatmap above shows the correlation between all numerical features in a single graph. The problem with heatmaps is that slight colour intensity variations can be hard to perceive by human eyes, especially with certain colour combinations. Approximately 8% of men and 0.5% of women have some form of colourblindness (BioTuringâ€™s Blog, 2018; Colour Blind Awareness, n.d.). The ggplot graph above has a colourblind friendly colour palette and an added numerical scale, allowing the viewer to check the exact correlation between the variables, as an example, the positive correlation between early and last bidding features is notorious. Another noticeable correlation is the negative one between the winning ratio and auction bids, for further investigation, both will be displayed in a scatter plot.

```{r, warning=FALSE}
ggplot(SBD, aes(x = Early_Bidding, y=Last_Bidding )) + 
    geom_point(size=2,alpha= 0.3) +
    theme_ipsum() +
    ggtitle("Correlation between Early and Last Bidding")
```


  The scatter plot makes it noticeable why those two variables are so positive correlated, considering that the last bidding value can never be lower than the early bidding value. A positive slope can be found in the diagonal centre of the 2-dimensional plot, this line represents the values which were the same in both features.

```{r, warning=FALSE}
ggplot(SBD, aes(x = Winning_Ratio, y=Auction_Bids )) + 
    geom_point(size=2,alpha= 0.3) +
    theme_ipsum()+
    ggtitle("Correlation between winning ratio and auction bids")
```

  Different from the previous scatter plot, this time the features auction bids and winning ratio, which had a negative correlation in the heatmap representation, do not show any visible correlation. This interaction will be later tested when used as a predictor in a linear regression method.  
  
  
```{r, warning=FALSE}
ggplot(SBD, aes(x= as.numeric(Record_ID))) +
  geom_histogram( binwidth=300, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  ggtitle("Record ID distribution") +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))

ggplot(SBD, aes(x= as.numeric(Auction_ID))) +
  geom_histogram( binwidth=60, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  ggtitle("Record ID distribution") +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))
```

The histograms above display a clear difference between the distribution of the Auction and Record ID, as the x-axis of the Auction ID, is much shorter, this happens because many auctions are represented in the data set multiple times. Also, the Record ID displays some uniformity, even so not perfectly uniform, representing the absence of multiple observations with the same ID.

```{r, warning=FALSE}
ggplot(melt_SBD, aes(x=variable, y=value, fill=variable )) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  theme_ipsum() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ggtitle("Box plot for multiple variables") +
  xlab("")
```

The Boxplot above shows the distribution of several numerical features with a range between 0 and 1. These 8 features were chosen to be represented by grouped boxplots since it is an exceptional alternative to viewing many distributions in a single graph, especially if the y-axis range matches all features. This figure is extremely informative, considering that it displays the distribution of the observations, the quartile range including the median, and the minimum and maximum values, which describe the range (Galarnyk, 2018).


```{r, warning=FALSE}
ggplot(SBD, aes(x= Auction_Duration)) +
  geom_histogram( binwidth=1, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  ggtitle("Auction Duration distribution") +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))
```

Despite being a numerical feature, the low number of distinct values causes the histogram to not be aesthetically pleasing, however, it shows the 5 distinct values 7,3,1,5 and 10 and how many times each one appeared in the auction duration column. This pattern suggests five fixed auction durations options that the platform allows the sellers to choose from.

```{r, results = 'asis'}
SBD[,4:13] %>%
pivot_wider(names_from = Class,
            values_from = c("Bidder_Tendency", "Bidding_Ratio", "Successive_Outbidding", "Last_Bidding","Auction_Bids", 
                            "Starting_Price_Average" , "Early_Bidding" ,
                            "Winning_Ratio" , "Auction_Duration" ), values_fn = ~mean(.x)) %>%
as.data.frame() %>% 
t() %>%
kable(format = "html")
```

The pivot table above points to a substantial difference in the mean value for most features when
compared to normal and abnormal bidding patterns. This information is important since the class
feature will be the dependable variable for machine learning models in the next steps.

# **Data cleaning**

The importance of identifying and removing missing values from the data prior to modelling is that it can reduce its accuracy and cause bias in the analysis (Analytics Vidhya, 2021). As shown before, there is not a single missing value in the data. However, sometimes the missing values appear as non-standard, a different format that includes strings named "NA", "n/a", "--" and many other possibilities. Unfortunately, it is near-impossible to check for all of them, especially in datasets with numerous observations and features. When it happens the feature type can only be categorical, since 0 in numeric features should not be considered meaningless (Sullivan, 2018). This section aims to check and remove non-standard missing values that may be present in the "Bidder_ID" column.

```{r}
SBD <- SBD %>%
  mutate(Bidder_ID = replace(Bidder_ID, Bidder_ID == "na", NA)) %>%
  mutate(Bidder_ID = replace(Bidder_ID, Bidder_ID == "N/A", NA)) %>%
  mutate(Bidder_ID = replace(Bidder_ID, Bidder_ID == "--", NA)) %>%
  mutate(Bidder_ID = replace(Bidder_ID, Bidder_ID == "n/a", NA)) 

sum(is.na(SBD$Bidder_ID))
```

Checking for common strings associated with non-standard missing values, the result of the analysis indicates the complete absence of them.

# **Feature engineering**

As an important stage preceding the machine learning modelling, this task requires the judgement of the data analyst and numerous trials and errors. Feature engineering is the act of transforming a given feature space with different tools, especially mathematical, with the main goal of improving the model performance (Khurana, Samulowitz and Turaga, 2018).The first step will be removing unnecessary columns for the future tasks, the first three columns only include the IDs for the record, auction and bidder respectively and do not add any significance to the analysis.

```{r}
SBD <- as.data.frame(SBD[,4:13])
head(SBD)
```


As observed by the correlation of early and last biding in the scatter plot, the early bidding can never exceed the last bidding in value, however, what eventually happens is that the auction has no increase in price and culminate in the same value. To join the information of the two columns in one and add extra information to the future predictive model, a new column named "Bidding difference" will substitute them with the existing incremented value calculated by the difference between their prices.

```{r}
SBD$Bidding_difference <- SBD$Last_Bidding - SBD$Early_Bidding
SBD <- as.data.frame(SBD[, c(-4,-7) ])
SBD <- SBD[order(SBD$Bidding_difference),]
head(SBD,30)
```

Calculating the difference between last and early bidding resulted in a feature with 25 negative values unexpected, suggesting that in these 25 observations the trade was concluded with the last bidding having a lower value than the early bidding, which is counterintuitive.

```{r}
SBD$Auction_Duration <- SBD$Auction_Duration / 10

ggplot(SBD, aes(x= Auction_Duration)) +
  geom_histogram( binwidth=0.1, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
  ggtitle("Scaled Auction Duration distribution") +
  theme_ipsum() +
  theme(plot.title = element_text(size=15))
```

Since all other metrics range from 0 to 1, dividing the auction duration variable by 10 puts it in the same range. This may reduce the chance of this variable disproportionately impacting the model.

# **Data scaling**

The different orders of magnitude may bias the results since most machine learning algorithms calculate the output using Euclidean distance (Asaithambi, 2017). As seen in the distribution of the previous boxplots and histograms the numeric features contain some outliers, considering the putative relevance of these outliers in the machine learning analysis, the ideal transformation method will maintain the impact of these feature outliers in the output of the machine learning algorithm to be performed. The two regularly used scaling methods are standardization and normalization, for this dataset the standardization will be applied as the transformation method of choice, since it copes better with outliers than normalization methods (GeeksforGeeks, 2020).

As displayed before, except for a few negative values in the Bidding difference column, all features range from 0 to 1. Data scaling is necessary when the features of a dataset have a different range or order of magnitude (Roy, 2020), which is not the case, taking that into consideration the data scaling may not be necessary for that instance. However, to evaluate that, the "X_s" variable will contain that scaled data to be compared to the not-scaled data.


```{r}
X <- SBD[,-8]
y <- SBD[,8]
X_s <- scale(X)
head(X)
head(y)
head(X_s)
```

The scaled variables are going to be had their performance measured in the feature reducing methods and the Random Forest Classification machine learning method to come, after the comparison, the one with better predictive power will be applied in the following tasks.

# **Linear Discriminant Analysis (LDA)**

In pattern classification using a Machine Learning algorithm, the training vectors are evaluated by comparing them to the test split, a common problem that can reduce its accuracy is the large dimensionality of the data, and numerous features reducing the robustness of the pattern classifiers. Linear discriminant analysis or LDA is a supervised technique that generally exceeds other techniques when applied to feature classification, LDA method reduces the number of features that fit in the different classes prior to classification while maintaining the classes well separated in this lower dimensional space (Sharma and Paliwal, 2014).
